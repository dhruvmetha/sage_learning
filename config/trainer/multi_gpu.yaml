_target_: lightning.pytorch.trainer.Trainer
accelerator: gpu
devices: auto  # Use all available GPUs
strategy: ddp  # Distributed Data Parallel
max_epochs: ${max_epochs}
precision: 16-mixed  # Mixed precision for faster training
gradient_clip_val: 1.0
log_every_n_steps: 10
check_val_every_n_epoch: ${check_val_every_n_epoch}
num_sanity_val_steps: 2

callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints
    monitor: val_loss
    mode: min
    save_top_k: 3
    save_last: true
    filename: "epoch{epoch:03d}-val_loss{val_loss:.4f}"
    auto_insert_metric_name: false
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: step
  # - _target_: lightning.pytorch.callbacks.EarlyStopping
  #   monitor: val_loss
  #   patience: 20
  #   mode: min

# logger:
#   - _target_: lightning.pytorch.loggers.TensorBoardLogger
#     save_dir: ${paths.output_dir}
#     name: ${name}
#     version: null
#     default_hp_metric: false
