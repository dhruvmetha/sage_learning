# Optimal multi-GPU training config for oct26 dataset
# 4x RTX A4000 (16GB each), 64 CPU cores, 15,812 training examples

defaults:
  - _self_
  - data: oct26_1push_data.yaml
  - model: diffusion_only_goal.yaml
  - trainer: multi_gpu.yaml

# Task name for logging
name: "oct26_1push_diffusion_only_goal"

# Output paths
paths:
  output_dir: ${oc.env:HOME}/Robotics/Mujoco/sage_learning/outputs/${name}
  log_dir: ${paths.output_dir}/logs

# Optimization settings
batch_size: 4  # Per GPU (effective batch = 4 * 4 * 2 = 32 with grad accumulation)
num_workers: 4  # Per GPU (total = 4 * 4 = 16 workers, safe for system limits)
base_lr: 0.0001  # Standard for DiT with large batch (used by model config)

# Seed for reproducibility
seed: 42

# GPU id for legacy scripts that set CUDA_VISIBLE_DEVICES (kept for compatibility)
gpu_id: 0

# Hydra settings
hydra:
  run:
    dir: ${paths.output_dir}
  job:
    chdir: false  # Don't change working directory (prevents import errors)
